{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import openai\n",
    "import tiktoken\n",
    "import dotenv\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure OpenAI API\n",
    "# os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_MODEL_NAME = os.environ.get(\"AZURE_MODEL_NAME\")\n",
    "AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_KEY = os.environ.get(\"AZURE_OPENAI_KEY\")\n",
    "AZURE_OPENAI_API_VERSION = os.environ.get(\"AZURE_OPENAI_API_VERSION\")\n",
    "AZURE_OPENAI_TEXT_DEPLOYMENT = os.environ.get(\"AZURE_OPENAI_TEXT_DEPLOYMENT\")\n",
    "AZURE_OPENAI_CHATGPT_DEPLOYMENT = os.environ.get(\"AZURE_OPENAI_CHATGPT_DEPLOYMENT\")\n",
    "AZURE_EMBEDDING_MODEL_NAME = os.environ.get(\"AZURE_EMBEDDING_MODEL_NAME\")\n",
    "openai.api_type = \"azure\"\n",
    "openai_api_type = 'azure'\n",
    "openai.api_version = AZURE_OPENAI_API_VERSION\n",
    "openai.api_base = AZURE_OPENAI_ENDPOINT\n",
    "openai.api_key = AZURE_OPENAI_KEY\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the location of the PDF file.\n",
    "pdfReader = PdfReader('../data/LLM-AI-Agents.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the text from the PDF file.\n",
    "raw_text = ''\n",
    "for i, page in enumerate(pdfReader.pages):\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TPTU: Task Planning and Tool Usage of\\nLarge Language Model-based AI Agents\\nJingqing Ruan†‡\\nruanjingqing@sensetime.comYihong Chen†‡\\nchenyihong@sensetime.comBin Zhang†‡\\nzhangbin11@sensetime.com\\nZhiwei Xu†‡\\nxuzhiwei@sensetime.comTianpeng Bao†\\nbaotianpeng@sensetime.comGuoqing Du†\\nduguoqing@sensetime.com\\nShiwei Shi†\\nshishiwei@sensetime.comHangyu Mao†∗\\nmaohangyu@sensetime.comXingyu Zeng\\nzengxingyu@sensetime.com\\nRui Zhao\\nzhaorui@sensetime.com\\nSenseTime Research, China\\nAbstract\\nWith recent advancements in natural language processing, Large Language Models\\n(LLMs) have emerged as powerful tools for various real-world applications. Despite\\ntheir prowess, the intrinsic generative abilities of LLMs may prove insufficient\\nfor handling complex tasks which necessitate a combination of task planning and\\nthe usage of external tools. In this paper, we first propose a structured framework\\ntailored for LLM-based AI Agents and discuss the crucial capabilities necessary for\\ntackling intricate problems. Withi'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first 1000 characters of the text.\n",
    "raw_text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into chunks of 1000 characters with 200 characters overlap.\n",
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "pdfTexts = text_splitter.split_text(raw_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show how many chunks of text are generated.\n",
    "len(pdfTexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the text chunks to the Embedding Model from Azure OpenAI API to generate embeddings.\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key = AZURE_OPENAI_KEY, \n",
    "    deployment = AZURE_OPENAI_TEXT_DEPLOYMENT, \n",
    "    model = AZURE_EMBEDDING_MODEL_NAME,\n",
    "    client=\"azure\",\n",
    "    chunk_size=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the text chunks to the Embedding Model from Azure OpenAI API to generate embeddings.\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_base= AZURE_OPENAI_ENDPOINT,\n",
    "    openai_api_type='azure',\n",
    "    deployment='text-embedding-ada-002',\n",
    "    openai_api_key=AZURE_OPENAI_KEY,\n",
    "    chunk_size=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use FAISS to index the embeddings. This will allow us to perform a similarity search on the texts using the embeddings.\n",
    "# https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/faiss.html\n",
    "pdfDocSearch = FAISS.from_texts(pdfTexts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use FAISS to index the embeddings. This will allow us to perform a similarity search on the texts using the embeddings.\n",
    "# https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/faiss.html\n",
    "pdfDocSearch = FAISS.from_texts(pdfTexts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory=None callbacks=None callback_manager=None verbose=False tags=None metadata=None input_key='input_documents' output_key='output_text' llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\", template_format='f-string', validate_template=True), llm=AzureOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.7, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='815e9449a52847f7acbff5391ed6e94a', openai_api_base='', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all', tiktoken_model_name=None, deployment_name='text-davinci-003', openai_api_type='azure', openai_api_version='2023-05-15'), output_key='text', output_parser=StrOutputParser(), return_final_only=True, llm_kwargs={}) document_prompt=PromptTemplate(input_variables=['page_content'], output_parser=None, partial_variables={}, template='{page_content}', template_format='f-string', validate_template=True) document_variable_name='context' document_separator='\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "# Create a Question Answering chain using the embeddings and the similarity search.\n",
    "# https://docs.langchain.com/docs/components/chains/index_related_chains\n",
    "chain = load_qa_chain(AzureOpenAI(openai_api_key=AZURE_OPENAI_KEY, deployment_name=AZURE_OPENAI_TEXT_DEPLOYMENT, model_name=AZURE_MODEL_NAME, openai_api_version=AZURE_OPENAI_API_VERSION), chain_type=\"stuff\")\n",
    "\n",
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This book discusses the use of techniques such as chain-of-thought and vector databases to endow AI Agents with the abilities of summarization, task planning, tool usage, perception, learning/reflection/memory, and summarization. The authors also explore phenomena such as LLMs difficulty in understanding output formats.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform first sample of question answering.\n",
    "inquiry = \"Please tell me the key summary of this book.\"\n",
    "docs = pdfDocSearch.similarity_search(inquiry)\n",
    "chain.run(input_documents=docs, question=inquiry)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
